# 逻辑回归


## 简介
就如线性回归是经典的回归模型一般，逻辑回归（Logistic Regression）是分类问题解决方案中比较经典的模型。不同于回归预测，分类问题要预测的值并非连续的，而是离散的。如果分类的目标（标签）只有两种取值（0或1，正例或负例），称为二分类问题。如果标签的取值在三种及三种以上，称为多分类问题。


## 逻辑回归
试图用线性回归这样的模型寻找分类的决策边界，会遇到很多问题，如难以控制输出值的范围。事实上，要想对这个模型得到可控制的输出值，可以将原来线性回归的输出值进行某种变换，使其值控制在[0, 1]之间。

$$
\begin{aligned} h_{\theta}(x) &=g\left(\theta^{T} x\right) \\ g(z) &=\frac{1}{1+e^{-z}} \end{aligned}
$$
因而，得到最终的假设函数（预测函数）如下，其输出值可以理解为类别的概率（更具体来说，输出的值为y=1的概率）。
$$
h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}
$$
上述的g函数将任何实数映射到[0, 1]之间，该函数称为Sigmoid函数或者Logistic函数，Sigmoid是经典的S型函数之一。![](./assets/sigmoid.png)
而训练的目标，就是找到最合适的参数$\theta$使得由$\theta$构成的模型能够最完美的将不同的类别划分开，找到的最合适的$\theta^{T}x$就称为决策边界（Decision Boundary）。越是复杂越是非线性的决策边界可以更好地进行分类，而决策边界并非数据集的属性而是假设函数和参数的属性。**获得更复杂更精准的决策边界，就是分类任务的目的。**


## 参数求解
